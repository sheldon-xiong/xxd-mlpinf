import os
import curses
import subprocess
import shutil
from typing import Dict, List, Optional, Tuple

from .window import run_form, TextDisplaySection, TextInputSection, SingleSelectSection, MultiSelectSection


# We don't want to require a docker container to run the GUI, and running the GUI within a container causes issues with
# launching remote jobs / interfacing with SLURM.
# We can't import from the `code` module directly since we are out-of-container, so use ast to parse
# code.common.constants instead.
import ast


with open("code/common/constants.py") as f:
    tdoc = ast.parse(f.read())


def _find_classdef(name: str):
    for node in ast.walk(tdoc):
        if isinstance(node, ast.ClassDef):
            if node.name == name:
                return node
    raise ValueError(f"Class definition {name} not found in code.common.constants")


def _enum_match_strs(class_name):
    cdef = _find_classdef(class_name)
    attr_nodes = []
    for child in ast.iter_child_nodes(cdef):
        if isinstance(child, ast.AnnAssign) and child.value.func.id == "AliasedName":
            attr_nodes.append(child)
    return [attr.value.args[0].value for attr in attr_nodes]


benchmark_names = _enum_match_strs("Benchmark")


# Likewise, we need to grab the system ID to use as a default. Luckily we have a handy-dandy script using a minimal
# Docker image to do so if we are out of container.
try:
    # We are in container, so we can just grab system ID normally.
    from code.common.systems.system_list import DETECTED_SYSTEM
    sys_id = DETECTED_SYSTEM.extras.get("id", "")
except ModuleNotFoundError:
    # We are out of container, so we need to grab system ID via a script.
    import subprocess
    sys_id = subprocess.run("bash scripts/get_system_id.sh",
                            shell=True,
                            capture_output=True,
                            text=True).stdout


def run_gui():
    sections = {
        "submitter_name": TextInputSection("Submitter Name:", "NVIDIA", allow_newlines=False),
        "system_name": TextInputSection("System Name:", sys_id.strip(), allow_newlines=False),
        "category": SingleSelectSection("Category:", ["Datacenter", "Edge"]),
        "benchmarks": MultiSelectSection("Select Benchmarks (space to toggle):", benchmark_names),
        "explicit_gen_engines": SingleSelectSection("Explicitly generate engines?", ["Disable", "Enable"]),
        "output_dir": TextInputSection("Output Directory:", "build/logs/default", allow_newlines=False),
    }

    return run_form("NVIDIA MLPerf Inference", sections)


class ScriptWriter:
    def __init__(self,
                 submitter_name: Optional[str] = None,
                 system_name: Optional[str] = None,
                 output_dir: Optional[str] = None,
                 explicit_gen_engines: bool = False,
                 category: str = "Datacenter"):
        self.header_lines = [
            "#!/bin/bash",
            "",
            "# This script is generated by the MLPerf Script Creator.",
            "",
            "set -euo pipefail",
            "source gui/script_utils.sh",
            "",
        ]

        self.explicit_gen_engines = explicit_gen_engines

        if submitter_name:
            self.header_lines.append(f"export SUBMITTER=\"{submitter_name}\"")
        if system_name:
            self.header_lines.append(f"export SYSTEM_NAME=\"{system_name}\"")
        if output_dir:
            self.header_lines.append(f"export LOG_DIR=\"{output_dir}\"")
        self.header_lines.append("export NO_DISPLAY_RESULTS=1")  # Don't display results until all workloads are done

        if category == "Datacenter":
            self.scenarios = ["Offline", "Server"]
        elif category == "Edge":
            self.scenarios = ["Offline", "SingleStream", "MultiStream"]
        else:
            raise ValueError(f"Invalid category: {category}")

        self.benchmark_fns = {}
        self.audit_lines = [
            "run_compliance_tests() {",
            "    make stage_results",
            "    rm -rf build/compliance_logs",
        ]

    def add_benchmark(self, benchmark_name: str):
        if benchmark_name.lower == "rgat":
            scenarios = ["Offline"]
        else:
            scenarios = self.scenarios

        # Sanitize benchmark name string to use as a bash function name
        fn_name = benchmark_name.lower().replace(" ", "_")
        fn_name = fn_name.replace("-", "_")
        fn_name = fn_name.replace(".", "_")
        fn_body = [
            f"{fn_name}() {{",
            f"    log_info \"Running {benchmark_name} workload in {scenarios} scenario(s)\"",
        ]

        for scenario in scenarios:
            if self.explicit_gen_engines:
                fn_body.append(f"    make generate_engines RUN_ARGS=\"--benchmarks={benchmark_name} --scenarios={scenario}\"")
            fn_body.append(f"    LOG_DIR=$LOG_DIR/PerformanceOnly make run_harness RUN_ARGS=\"--benchmarks={benchmark_name} --scenarios={scenario}\"")
            fn_body.append(f"    LOG_DIR=$LOG_DIR/AccuracyOnly make run_harness RUN_ARGS=\"--benchmarks={benchmark_name} --scenarios={scenario} --test_mode=AccuracyOnly\"")
            self.audit_lines.append(f"    make run_audit_harness RUN_ARGS=\"--benchmarks={benchmark_name} --scenarios={scenario}\"")

            if benchmark_name in ("bert", "dlrm-v2", "gptj", "llama2-70b"):  # TODO: This should be grabbed via ast.parse.
                if self.explicit_gen_engines:
                    fn_body.append(f"    make generate_engines RUN_ARGS=\"--benchmarks={benchmark_name} --scenarios={scenario} --accuracy_target=0.999\"")
                fn_body.append(f"    LOG_DIR=$LOG_DIR/PerformanceOnly make run_harness RUN_ARGS=\"--benchmarks={benchmark_name} --scenarios={scenario} --accuracy_target=0.999\"")
                fn_body.append(f"    LOG_DIR=$LOG_DIR/AccuracyOnly make run_harness RUN_ARGS=\"--benchmarks={benchmark_name} --scenarios={scenario} --accuracy_target=0.999 --test_mode=AccuracyOnly\"")

                self.audit_lines.append(f"    make run_audit_harness RUN_ARGS=\"--benchmarks={benchmark_name} --scenarios={scenario} --accuracy_target=0.999\"")
        fn_body.append("}")
        self.benchmark_fns[benchmark_name] = fn_body

    def generate_pack_submission(self):
        lines = [
            "pack_submission() {",
            "    make truncate_results",
            "}",
        ]
        return lines

    def generate_main(self):
        """Generate the main function that routes to benchmark functions based on command line arguments."""
        main_lines = [
            "# Function: all",
            "# Description: Run all benchmark functions",
            "all() {",
            "    log_info \"Running all benchmarks...\"",
        ]

        # Add calls to all benchmark functions in the 'all' function
        for benchmark_name, fn_body in self.benchmark_fns.items():
            fn_name = fn_body[0][:-4]  # Remove " () {" to get function name
            main_lines.append(f"    {fn_name}")

        main_lines.extend([
            "    log_success \"All benchmarks completed successfully!\"",
            "    display_results",
            "}",
            "",
            "# Function: display_results",
            "# Description: Display results from completed benchmark runs",
            "display_results() {",
            "    make display_results",
            "}",
            "",
            "# Function: help",
            "# Description: Display usage information",
            "help() {",
            "    cat << EOF",
            "Usage: $0 <command> [arguments...]",
            "",
            "Available commands:",
        ])

        # Add help text for each benchmark using benchmark names
        for benchmark_name in self.benchmark_fns.keys():
            main_lines.append(f"    {benchmark_name:<20} - Run {benchmark_name} benchmark")

        main_lines.extend([
            "    all                  - Run all benchmarks",
            "    display_results      - Display results from completed benchmark runs",
            "    run_compliance_tests - Run compliance tests. ONLY RUN THIS AFTER ALL BENCHMARKS ARE RUN.",
            "    help                 - Display this help message",
            "",
            "Examples:",
        ])

        # Add example usage for each benchmark using benchmark names
        for benchmark_name in self.benchmark_fns.keys():
            main_lines.append(f"    $0 {benchmark_name}")

        main_lines.extend([
            "    $0 all",
            "    $0 display_results",
            "    $0 help",
            "",
            "EOF",
            "}",
            "",
            "# Main function to route commands",
            "main() {",
            "    # Check if no arguments provided",
            "    if [ $# -eq 0 ]; then",
            "        log_error \"No command specified\"",
            "        help",
            "        exit 1",
            "    fi",
            "",
            "    local command=\"$1\"",
            "    shift  # Remove the command from arguments list",
            "",
            "    # Route to appropriate function",
            "    case \"$command\" in",
        ])

        # Add case statements using benchmark names as the command
        for benchmark_name, fn_body in self.benchmark_fns.items():
            fn_name = fn_body[0][:-4]  # Remove " () {" to get function name
            main_lines.extend([
                f"        \"{benchmark_name}\")",
                f"            {fn_name} \"$@\"",
                "            ;;",
            ])

        # Add all, help, and default cases
        main_lines.extend([
            "        \"all\")",
            "            all \"$@\"",
            "            ;;",
            "        \"display_results\")",
            "            display_results \"$@\"",
            "            ;;",
            "        \"run_compliance_tests\")",
            "            run_compliance_tests \"$@\"",
            "            ;;",
            "        \"help\"|\"-h\"|\"--help\")",
            "            help",
            "            ;;",
            "        *)",
            "            log_error \"Unknown command: $command\"",
            "            echo",
            "            help",
            "            exit 1",
            "            ;;",
            "    esac",
            "}",
            "",
            "# Only run main if script is executed directly (not sourced)",
            "if [[ \"${BASH_SOURCE[0]}\" == \"${0}\" ]]; then",
            "    main \"$@\"",
            "fi"
        ])

        return main_lines


    def write_script(self, path: os.PathLike):
        all_lines = self.header_lines

        for benchmark_name, fn_body in self.benchmark_fns.items():
            all_lines.extend(fn_body)
            all_lines.append("")

        self.audit_lines.append("    make stage_compliance")
        self.audit_lines.append("}")
        all_lines.extend(self.audit_lines)
        all_lines.append("")

        all_lines.extend(self.generate_main())

        with open(path, "w") as f:
            f.write("\n".join(all_lines))


def main():
    """Run the interactive form and return the user's selections."""
    opts = run_gui()
    if not opts:
        return "No options selected"

    writer = ScriptWriter(submitter_name=opts["submitter_name"],
                          system_name=opts["system_name"],
                          output_dir=opts["output_dir"],
                          explicit_gen_engines=(opts["explicit_gen_engines"] == "Enable"),
                          category=opts["category"])
    for benchmark_name in opts["benchmarks"]:
        writer.add_benchmark(benchmark_name)

    script_path = f"/tmp/run_{opts['submitter_name']}_{opts['system_name']}.sh"
    writer.write_script(script_path)
    resp = input(f"Would you like to edit / review the script? (y/N): ")
    if resp.lower().startswith("y"):
        editor_cmd = os.environ.get("EDITOR",
                                    os.environ.get("GIT_EDITOR", "nano"))
        subprocess.run([editor_cmd, script_path], check=True)


    resp = input(f"Script name (Default: {os.path.basename(script_path)}): ")
    if not resp:
        target_path = os.path.join(os.getcwd(), os.path.basename(script_path))
    else:
        target_path = os.path.join(os.getcwd(), resp)

    shutil.move(script_path, target_path)
    subprocess.run(["chmod", "+x", target_path], check=True)
    print(f"Script moved to: {target_path}")


if __name__ == "__main__":
    main()
