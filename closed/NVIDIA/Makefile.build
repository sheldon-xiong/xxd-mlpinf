# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#	 http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# This Makefile contains all the variable and targets related to building the binaries for NVIDIA's MLPerf Inference
# submission. This includes TensorRT plugins, C++ harness binaries, and Triton Inference Server.

ifndef MAKEFILE_BUILD_INCLUDED
MAKEFILE_BUILD_INCLUDED := 1

include $(CURDIR)/Makefile.const
include $(CURDIR)/Makefile.power

# Specify debug options for build (default to Release build)
ifeq ($(DEBUG),1)
	BUILD_TYPE := Debug
else
	BUILD_TYPE := Release
endif

# Set the include directory for Loadgen header files
INFERENCE_DIR = $(BUILD_DIR)/inference
INFERENCE_URL = https://github.com/mlcommons/inference.git
LOADGEN_INCLUDE_DIR ?= $(INFERENCE_DIR)/loadgen
LOADGEN_LIB_DIR ?= $(LOADGEN_INCLUDE_DIR)/build
# Loadgen hash: 7/8/2025
INFERENCE_HASH = 24767db549fb6cf0cd506113e34a2a8402ea222f

# Set Environment variables to extracted contents
export LD_LIBRARY_PATH := $(LD_LIBRARY_PATH):/usr/local/cuda/lib64:/usr/lib/$(ARCH)-linux-gnu:$(LOADGEN_LIB_DIR)
export HARNESS_LD_LIBRARY_PATH := $(LD_LIBRARY_PATH)	# LD_LIBRARY_PATH used during run_harness
export LIBRARY_PATH := /usr/local/cuda/lib64:/usr/lib/$(ARCH)-linux-gnu:$(LOADGEN_LIB_DIR):$(LIBRARY_PATH)
export PATH := /usr/local/cuda/bin:$(PATH)
export CPATH := /usr/local/cuda/include:/usr/include/$(ARCH)-linux-gnu:/usr/include/$(ARCH)-linux-gnu/cub:$(CPATH)
export CUDA_PATH := /usr/local/cuda
export CCACHE_DISABLE=1
export NUMBA_CACHE_DIR=$(BUILD_DIR)/cache

# Set CUDA_DEVICE_MAX_CONNECTIONS to increase multi-stream performance.
export CUDA_DEVICE_MAX_CONNECTIONS := 32


# FFI Utils for Python
FFI_UTILS_DIR := $(BUILD_DIR)/harness/lib

# mitten hash:
MITTEN_PUBLIC_HASH := be989cf09fccf4eb391f3d17c81b28d1eed30fc2
MITTEN_PUBLIC_GIT_URL := https://github.com/NVIDIA/mitten.git
MITTEN_DEV_HASH := 82a930d962ce6bd8aed38cf185a2acfbbfd6b84b
MITTEN_GIT_URL := ssh://git@gitlab-master.nvidia.com:12051/mlpinf/mitten.git

# Set the Triton directory
TRITON_DIR = $(BUILD_DIR)/triton-inference-server
TRITON_OUT_DIR = $(TRITON_DIR)/out
TRITON_PREBUILT_LIBS_DIR = prebuilt_triton_libs
TRITON_URL = https://github.com/triton-inference-server/server
TRITON_TEKIT_URL = ssh://git@gitlab-master.nvidia.com:12051/ftp/tekit_backend.git
TRITON_TRTLLM_URL = https://github.com/triton-inference-server/tensorrtllm_backend.git
TRITON_HASH = r24.12
TRITON_SERVER_COMMIT = 1efbd53
TRITON_TRTLLM_BRANCH = main
TRITON_TEKIT_BRANCH = release/0.17

# 2/13/2025 GitHub release
TRITON_TRTLLM_HASH ?= f51ea9dca26778ff2945344b9b1e413d5a34ebed
TRITON_COMMON_HASH = $(TRITON_HASH)
TRITON_CORE_HASH = $(TRITON_HASH)
TRITON_BACKEND_HASH = $(TRITON_HASH)
TRITON_THIRDPARTY_HASH = $(TRITON_HASH)
TRITON_TENSORRT_HASH = $(TRITON_HASH)
TRITON_PYTHON_BE_HASH = $(TRITON_HASH)

TRT_INCLUDE_DIR = /usr/include/$(ARCH)-linux-gnu/

# Set this to 0 once repo is frozen
CHECK_TRITON_VERSION=0
BYPASS_TRITON_WARNING=0


ifneq (,$(filter $(SM),80 87 89 90 100))
    BUILD_TRTLLM ?= 1
    ifneq (,$(filter $(SM),90 100))
        TRTLLM_BUILD_ARCH ?= 90-real;100-real
    else
        TRTLLM_BUILD_ARCH ?= 90-real;100-real;$(SM)-real
    endif
else
     BUILD_TRTLLM ?= 0
endif

# Whether to use public TRTLLM on github
TRTLLM_DIR ?= $(BUILD_DIR)/TRTLLM
TRTLLM_BUILD_DIR ?= $(BUILD_DIR)/TRTLLM/build
TRTLLM_URL = https://github.com/NVIDIA/TensorRT-LLM.git
# June 26 2025: support nvfp4 model and fp8 kv cache for MLA chunked prefill (Blackwell) (#5475)
TRTLLM_HASH = 8836990bde20a4d3fc64a2aabe78b427e6899120


BUILD_TRITON ?= 0

# Build flags
HARNESS_BUILD_FLAGS := -DCMAKE_BUILD_TYPE=$(BUILD_TYPE) \
                       -DLOADGEN_INCLUDE_DIR=$(LOADGEN_INCLUDE_DIR) \
                       -DLOADGEN_LIB_DIR=$(LOADGEN_LIB_DIR) \
                       -DFFI_UTILS_DIR=$(FFI_UTILS_DIR) \
                       -DCMAKE_EXPORT_COMPILE_COMMANDS=ON

TRITON_BUILD_FLAGS := --cmake-dir=$(TRITON_DIR) \
                      --build-dir=$(TRITON_OUT_DIR) \
                      --repo-tag=common:$(TRITON_COMMON_HASH) \
                      --repo-tag=core:$(TRITON_CORE_HASH) \
                      --repo-tag=backend:$(TRITON_BACKEND_HASH) \
                      --repo-tag=thirdparty:$(TRITON_THIRDPARTY_HASH) \
                      --backend=tensorrtllm:$(TRITON_TRTLLM_BRANCH) \
                      --enable-logging \
                      --no-container-build \
                      --enable-gpu \
                      --endpoint=http \
                      --endpoint=grpc \
                      --no-force-clone \
                      --enable-metrics \
                      --enable-stats
ifeq ($(DEBUG),1)
    TRITON_BUILD_FLAGS += --backend=ensemble --enable-tracing --enable-nvtx
endif

ifeq ($(IS_SOC)$(ARCH), 0aarch64)
    BUILD_TRITON := 0
else
    # Jetson
    ifeq ($(IS_SOC), 1)
            BUILD_TRITON := 0
    endif
    HARNESS_BUILD_FLAGS += -DBUILD_TRITON=$(BUILD_TRITON) -DIS_SOC=$(IS_SOC) -DSOC_SM=$(SOC_SM)
    TRITON_BUILD_FLAGS += --build-type=$(BUILD_TYPE)
endif

PLUGINS := NMSOptPlugin retinanetConcatPlugin DLRMv2EmbeddingLookupPlugin
PLUGIN_TARGETS := $(addprefix plugin., $(PLUGINS))

# Add symbolic links to scratch path if it exists.
.PHONY: link_dirs
link_dirs:
	@mkdir -p build
	@mkdir -p $(DATA_DIR)
	@mkdir -p $(PREPROCESSED_DATA_DIR)
	@mkdir -p $(MODEL_DIR)
	@ln -sfn $(DATA_DIR) $(DATA_DIR_LINK)
	@ln -sfn $(PREPROCESSED_DATA_DIR) $(PREPROCESSED_DATA_DIR_LINK)
	@ln -sfn $(MODEL_DIR) $(MODEL_DIR_LINK)

# Clone Triton.
.PHONY: clone_triton
clone_triton:
	@if [ ! -d $(TRITON_DIR) ]; then \
		echo "Cloning Triton Inference Server"; \
		git clone $(TRITON_URL) $(TRITON_DIR); \
	fi

	@cd $(TRITON_DIR) && \
		git fetch && \
		git reset --hard HEAD && \
		git checkout $(TRITON_SERVER_COMMIT)

	@mkdir -p $(TRITON_OUT_DIR)

	@if [ "$(USE_RELEASE_TRTLLM)" -eq 0 ]; then \
		if [ ! -d $(TRITON_OUT_DIR)/tensorrtllm ]; then \
			git clone -b $(TRITON_TEKIT_BRANCH) $(TRITON_TEKIT_URL) $(TRITON_OUT_DIR)/tensorrtllm; \
		else \
			echo "Updating existing tensorrtllm repository"; \
			cd $(TRITON_OUT_DIR)/tensorrtllm && git fetch && git reset --hard HEAD && git checkout $(TRITON_TEKIT_BRANCH); \
		fi; \
	else \
		if [ ! -d $(TRITON_OUT_DIR)/tensorrtllm ]; then \
			git clone $(TRITON_TRTLLM_URL) $(TRITON_OUT_DIR)/tensorrtllm && \
			cd $(TRITON_OUT_DIR)/tensorrtllm && \
			git checkout $(TRITON_TRTLLM_HASH); \
		else \
			echo "Updating existing tensorrtllm repository"; \
			cd $(TRITON_OUT_DIR)/tensorrtllm && git fetch && git reset --hard HEAD && git checkout $(TRITON_TRTLLM_HASH); \
		fi; \
	fi

	@rm -rf $(TRITON_OUT_DIR)/tensorrtllm/tensorrt_llm
	@ln -s $(BUILD_DIR)/TRTLLM/ $(TRITON_OUT_DIR)/tensorrtllm/tensorrt_llm

	@rm -rf $(TRITON_OUT_DIR)/trt_link
	@mkdir -p $(TRITON_OUT_DIR)/trt_link
	@ln -s $(TRT_INCLUDE_DIR) $(TRITON_OUT_DIR)/trt_link/include

	@cd $(TRITON_DIR) && \
		git apply $(PROJECT_ROOT)/scripts/mlperf_triton_installation.patch

	@cd $(TRITON_OUT_DIR)/tensorrtllm/inflight_batcher_llm && \
		git apply $(PROJECT_ROOT)/scripts/mlperf_triton_trtllm_backend_installation.patch

# Build all source codes (needed in internal dev mode).
.PHONY: build
build: clone_loadgen clone_power_dev link_dirs
ifeq ($(BUILD_TRITON), 1)
	@$(MAKE) -f Makefile.build clone_triton
	@$(MAKE) -f Makefile.build build_triton
endif
ifeq ($(BUILD_TRTLLM), 1)
	@$(MAKE) -f Makefile.build clone_trt_llm
	@$(MAKE) -f Makefile.build build_trt_llm
endif
	@$(MAKE) -f Makefile.build build_loadgen
	@$(MAKE) -f Makefile.build build_plugins
	@$(MAKE) -f Makefile.build build_harness

# Clone LoadGen repo.
.PHONY: clone_loadgen
clone_loadgen:
	@if [ ! -d $(LOADGEN_INCLUDE_DIR) ]; then \
		echo "Cloning Official MLPerf Inference (For Loadgen Files)" \
			&& git clone $(INFERENCE_URL) $(INFERENCE_DIR); \
	fi
	@echo "Updating Loadgen" \
		&& cd $(INFERENCE_DIR) \
		&& git fetch \
		&& git checkout $(INFERENCE_HASH) \
		&& git submodule update --init --recursive


.PHONY: clone_trt_llm
clone_trt_llm:
	@echo "Cloning TRT LLM in $(TRTLLM_DIR) from $(TRTLLM_URL)"
	@if [ ! -d $(TRTLLM_DIR) ]; then \
		echo "Cloning TRT-LLM" \
			&& git clone $(TRTLLM_URL) $(TRTLLM_DIR); \
	fi
	@echo "Updating TRT-LLM to $(TRTLLM_HASH)" \
		&& cd $(TRTLLM_DIR) \
		&& git fetch \
		&& git reset --hard HEAD \
		&& git checkout $(TRTLLM_HASH) \
		&& git submodule update --init --recursive


# Build Triton.
.PHONY: build_triton
build_triton:
	@echo "Building TensorRT Inference Server..."
	@if [ ! -d $(TRITON_DIR) ]; then \
		echo "triton-inference-server does not exist! Please run make clone_triton." \
			&& exit 1; \
	fi
	# Required till triton build.py properly supports incremental builds
	@echo "Build command: ./build.py $(TRITON_BUILD_FLAGS)"
	@rm -rf $(TRITON_OUT_DIR)/tensorrtllm/tensorrt_llm \
		&& ln -s $(BUILD_DIR)/TRTLLM/ $(TRITON_OUT_DIR)/tensorrtllm/tensorrt_llm \
		&& cd $(TRITON_DIR) \
		&& ./build.py $(TRITON_BUILD_FLAGS)


# Build TRT-LLM wheels and librarys in llm specific virtual environment.
.PHONY: build_trt_llm
build_trt_llm:
	# Create the TRT-LLM development virtual environment from scratch
	@if [ -d .llm_$(ARCH) ]; then \
		echo "Virtual environment .llm_$(ARCH) already exists, remove and create new one" \
			&& rm -rf .llm_$(ARCH); \
	fi
	python -m venv .llm_$(ARCH) --system-site-packages
	@echo "Building TRT-LLM..."
	cd $(TRTLLM_DIR) && CCACHE_NODISABLE=true /work/.llm_$(ARCH)/bin/python scripts/build_wheel.py --use_ccache --benchmarks -a="$(TRTLLM_BUILD_ARCH)" --no-venv --clean
	/work/.llm_$(ARCH)/bin/pip install $(TRTLLM_BUILD_DIR)/tensorrt*.whl
	/work/.llm_$(ARCH)/bin/pip install -r docker/common/requirements/requirements.llm.txt
	@echo "Checking for Mixtral fp4-quantized model archive in /opt..."
	@if [ -d "$(BUILD_DIR)/models/Mixtral/fp4-quantized-modelopt/mixtral-8x7b-instruct-v0.1-tp1pp1-fp4-e7.25-passing_acuracies" ]; then \
		echo "Checkpoint directory already exists at $(BUILD_DIR)/models/Mixtral/fp4-quantized-modelopt/mixtral-8x7b-instruct-v0.1-tp1pp1-fp4-e7.25-passing_acuracies, skipping extraction."; \
	elif [ -f "/opt/fp4-quantized-modelopt/mixtral-8x7b-instruct-v0.1-tp1pp1-fp4-e7.25.tar.gz" ]; then \
		echo "Found model archive at /opt/fp4-quantized-modelopt/mixtral-8x7b-instruct-v0.1-tp1pp1-fp4-e7.25.tar.gz, extracting..."; \
		mkdir -p $(BUILD_DIR)/models/Mixtral/fp4-quantized-modelopt/; \
		tar -xzf /opt/fp4-quantized-modelopt/mixtral-8x7b-instruct-v0.1-tp1pp1-fp4-e7.25.tar.gz -C $(BUILD_DIR)/models/Mixtral/fp4-quantized-modelopt/; \
	else \
		echo "Model archive not found in /opt, skipping extraction."; \
	fi

# Build TensorRT plugins.
.PHONY: build_plugins
build_plugins: $(PLUGIN_TARGETS)


.PHONY: $(PLUGIN_TARGETS)
$(PLUGIN_TARGETS): plugin.%: code/plugin/%
	mkdir -p build/plugins/$(<F)
	cd build/plugins/$(<F)\
		&& cmake -DCMAKE_BUILD_TYPE=$(BUILD_TYPE) $(PROJECT_ROOT)/code/plugin/$(<F) \
		&& make -j


# Build LoadGen.
.PHONY: build_loadgen
build_loadgen:
	@echo "Building loadgen..."
	@if [ ! -e $(LOADGEN_LIB_DIR) ]; then \
		mkdir -p $(LOADGEN_LIB_DIR); \
	fi
	@if ls $(LOADGEN_INCLUDE_DIR)/mlcommons_*.whl 1> /dev/null 2>&1; then \
		echo "Cleaning up old wheels...." \
		&& rm $(LOADGEN_INCLUDE_DIR)/mlcommons_*.whl; \
	fi
	@cd $(LOADGEN_LIB_DIR) \
		&& cmake -DCMAKE_POSITION_INDEPENDENT_CODE=ON -DCMAKE_BUILD_TYPE=$(BUILD_TYPE) .. \
		&& make -j
	@cd $(LOADGEN_INCLUDE_DIR) \
		&& CFLAGS="-std=c++14 -O3" pip wheel . \
		&& pip install --user --force-reinstall mlcommons_loadgen*.whl


# Build harness source codes.
.PHONY: build_harness
build_harness:
	@echo "Building harness..."
	@mkdir -p build/harness \
		&& cd build/harness \
		&& cmake -DPYTHON3_CMD=$(PYTHON3_CMD) $(HARNESS_BUILD_FLAGS) $(PROJECT_ROOT)/code/harness \
		&& make -j
	@echo "Finished building harness."

endif # ifndef MAKEFILE_BUILD_INCLUDED
