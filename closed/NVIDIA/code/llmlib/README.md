# Core functionality for LLMs

## Targeted workloads

This supports all language models in mlperf-inference and is the default choice for running workloads. This includes:

- llama2-70b
- mixtral-8x7b
- llama2-70b-interactive
- llama3.1-405b

## Integration with NVIDIA's MLPerf Inference pipeline

NVIDIA's MLPerf Inference is primarily built on top of nv-mitten, and the `llmlib` submodule provides implementations of core nv-mitten pipeline operations for the major steps of LLM benchmark execution:

1. Model quantization (also known as quantized checkpoint generation, or just the 'checkpoint' step)
2. TRT engine generation
3. Harness execution

### File structure

- `builder.py`: Operations for checkpoint and engine generation
- `cores/`: Directory containing all backend implementations
  - `base.py`: Abstract `LLMCore` base class defining the interface for all backends
  - `trtllm_executor_core.py`: TensorRT-LLM Executor backend implementation
  - `triton_grpc_core.py`: Triton gRPC client backend implementation
  - `trtllm_endpoint_core.py`: TRT-LLM HTTP endpoint backend implementation
  - `trtllm_hlapi_core.py`: TRT-LLM high-level API backend implementation (not yet fully implemented)
  - `dummy.py`: Dummy backend for testing
  - `__init__.py`: Backend registry for dynamic backend discovery
- `server.py`: `LLMServer`, a class that load balances queries across a pool of `LLMCore`s
- `dataset.py`: `LLMDataLoader`, a base class for loading datasets for LLM models to perform inference on
- `config.py`: Abstraction objects for TRTLLM configuration options
- `fields.py`: nv-mitten configuration fields
- `factory.py`: `LLMServerFactory` for creating complete server instances with all components
- `warmup.py`: `WarmupManager` for parallel warmup and health checks
- `launch_server.py`: Operations for launching Triton and trtllm-serve servers
- `__init__.py`: `LLMHarnessOp` and variant harness operations for running MLPerf Inference loadgen tests
- `utils.py`: Utility objects used by the rest of `llmlib`


### Configuring workloads

In addition to the normal configuration workflow and fields, `llmlib.fields` provides specific configuration settings for each stage of LLM execution.

1. Checkpoint generation / quantization
    - `--llm_quantizer_outdir`: Path to store the quantized checkpoints for the model
    - `--quantizer_lib_path_override`: Path to the TRTLLM library to use for the quantization step. Useful for workloads where the quantization step has a custom implementation.
    - `--trtllm_checkpoint_flags`: Accepts a string in the format `key1:value1,key2:value2,...`
        - The `TRTLLM_CHECKPOINT_FLAGS` environment variable in the same string format. These are applied on top of the existing `trtllm_checkpoint_flags` Field value.
2. TRT Engine generation
    - `--tensor_parallelism` and `--pipeline_parallelism` for distributed workload configuration
    - `--moe_expert_parallelism` for expert parallelism in MoE models
    - `--trtllm_build_flags`: Accepts a string in the format `key1:value1,key2:value2,...`
        - The `TRTLLM_BUILD_FLAGS` environment variable in the same string format. These are applied on top of the existing `trtllm_build_flags` Field value.
3. Harness execution
    - `--llm_gen_config_path`: Path to the JSON config files generated by TRTLLM during engine generation
    - `--trtllm_runtime_flags`: Accepts a string in the format `key1:value1,key2:value2,...`
        - The `TRTLLM_RUNTIME_FLAGS` environment variable in the same string format. These are applied on top of the existing `trtllm_runtime_flags` Field value.
    - `--triton_server_urls`: Comma-separated list of Triton server URLs (e.g., `host1:port1,host2:port2`)
    - `--trtllm_server_urls`: Comma-separated list of TRTLLM server URLs for endpoint-based inference
    - `--triton_num_clients_per_server`: Number of gRPC clients to use per server (default=1)
    - `--triton_num_models_per_server`: Number of models to load on each Triton server (default=1)
    - `--show_steady_state_progress`: Show steady state throughput information in progress bar


**Note**: These TRTLLM flags are unfortunately somewhat arbitrary and vary across all models.

### Configuration value precedence

Let's say we have some class `LLMThing` that takes in `trtllm_runtime_flags` and is properly set up to use nv-mitten's `@autoconfigure`:

```
@autoconfigure
@bind(code.llmlib.fields.trtllm_runtime_flags, "flags")
class LLMThing:
    def __init__(self,
                 *args,
                 flags: Optional[Dict[str, Any]] = None,
                 **kwargs):
        ....
```

The value that will be assigned to `flags` is searched for in the following order:

1. If it is set directly in the object instantiation call, this value will be used. For example:
```
x = LLMThing(flags={"foo": "bar"})  # `flags` will be {"foo": "bar"} no matter what
```
2. If it is set in the command line as the `--trtllm_runtime_flags` flag, this value will be used. For example:
```
# Invoked with --trtllm_runtime_flags="n_tokens:1337,my_cool_field:foobar"

config = {code.llmlib.fields.trtllm_runtime_flags: {"n_tokens": 42,
                                                    "other_key": "value"}}

with Configuration(config).autoapply():
    x = LLMThing()  # `flags` will be {"n_tokens": 1337, "my_cool_field": "foobar"}
```
3. If it is in the current context's configuration (i.e. via `.autoapply()`), this value will be used. For example:
```
# --trtllm_runtime_flags is NOT in the command line flags

config = {code.llmlib.fields.trtllm_runtime_flags: {"n_tokens": 42,
                                                    "other_key": "value"}}

with Configuration(config).autoapply():
    x = LLMThing()  # `flags` will be {"n_tokens": 42, "other_key": "value"}
```

If you are using the provided LLM objects (`builder.QuantizerConfig`, `builder.TRTLLMBuilderOp`, and `config.TrtllmHarnessConfig`), then there will be some additional behavior:

1. `builder.QuantizerConfig` will apply the value supplied by the `TRTLLM_CHECKPOINT_FLAGS` environment variable on top of the value of `fields.trtllm_checkpoint_flags` via a dictionary update.
2. `builder.TRTLLMBuilderOp` will apply the value supplied by the `TRTLLM_BUILD_FLAGS` environment variable on top of the value of `fields.trtllm_build_flags` via a dictionary update.
3. `config.TrtllmHarnessConfig` starts out with a default value:
```
{
    'batch_scheduler_policy': 'max_util',
    'context_chunking_policy': 'first_come_first_served',
    'use_inflight_batching': True,
    'enable_batch_size_tuning': False,
    'enable_max_num_tokens_tuning': False,
    'dynamic_batch_moving_average_window': 128,
    'kvcache_free_gpu_mem_frac': 0.80,
    'enable_chunked_context': False,
    'exclude_input_from_output': True,
}
```
and then applies the value of `fields.trtllm_runtime_flags` on top of this default. After this, if the `TRTLLM_RUNTIME_FLAGS` environment variable is set, then it will be applied on top to get the final TRTLLM runtime options.

### Configuration System

Each backend uses a hierarchical configuration system:

```python
# Base configuration for all backends
HarnessConfig:
  - tensor_parallelism, pipeline_parallelism, moe_expert_parallelism
  - traffic_distribution_policy (for load balancing between DP ranks)
  - gen_config (GenerationConfig) (model specific generation_config.json)

# TensorRT-LLM Executor specific extensions
TrtllmExecutorConfig(TrtllmHarnessConfig):
  - engine_dir: Path to TRT engine directory
  - engine_config: Loaded TRT-LLM engine configuration

# TensorRT-LLM common extensions
TrtllmHarnessConfig(HarnessConfig):
  - checkpoint_flags: Model quantization settings
  - build_flags: TensorRT engine build parameters
  - runtime_flags: Executor runtime configuration

# Triton specific extensions
TritonHarnessConfig(TrtllmHarnessConfig):
  - server_url: Triton server endpoint
  - model_name: Target model identifier
  - clients_per_server: Concurrency settings

# TRT-LLM Endpoint specific extensions
TrtllmEndpointConfig(TrtllmHarnessConfig):
  - model_name: Model identifier
  - endpoint_url: HTTP endpoint URL
  - max_concurrency: Maximum concurrent requests

# TRT-LLM High-level API specific extensions
TrtllmHlApiConfig(TrtllmHarnessConfig):
  - model_name: Model path or HuggingFace identifier
  - backend_type: Backend engine type (pytorch or trt)

```

### Adding New Backends

To add a new inference backend:

1. **Implement `LLMCore` interface**: Create a new class inheriting from `LLMCore` in `cores/`
2. **Implement required abstract methods**:
   - `_enqueue_impl()`: Submit requests to your backend
   - `_poll_responses_impl()`: Retrieve responses from your backend
   - `run_health_check()`: Health check for backend readiness
   - `get_num_cores_for_workload()`: Calculate number of core instances needed
   - `get_config_for_core()`: Generate configuration for each core instance
3. **Implement lifecycle methods**:
   - `warm_up()`: Run warmup queries (optional, base class has default)
   - `flush()`: Flush all pending queries (optional, base class has default)
   - `_cleanup_resources()`: Clean up backend resources (optional)
4. **Create configuration class**: Extend `HarnessConfig` with backend-specific settings
5. **Register the backend**: Update `BackendRegistry` class with new module path
6. **Add harness operation**: Create a corresponding harness op in `__init__.py` if needed
